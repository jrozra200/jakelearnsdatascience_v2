<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Jake Learns Data Science</title>
    <link>/tags/r/</link>
    <description>Recent content in R on Jake Learns Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Jacob Rozran</copyright>
    <lastBuildDate>Mon, 14 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Visualizing Exercise Data from Strava</title>
      <link>/post/visualizing-exercise-data-from-strava/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-exercise-data-from-strava/</guid>
      <description>INTRODUCTION My wife introduced me to cycling in 2014 - I fell in love with it and went all in. That first summer after buying my bike, I rode over 500 miles (more on that below). My neighbors at the time, also cyclists, introduced me to the app Strava. Ever since then, I’ve tracked all of my rides, runs, hikes, walks (perhaps not really exercise that needs to be tracked… but I hurt myself early in 2018 and that’s all I could do for a while), etc.</description>
    </item>
    
    <item>
      <title>Prime Number Patterns</title>
      <link>/post/prime-number-patterns/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/prime-number-patterns/</guid>
      <description>I found a very thought provoking and beautiful visualization on the D3 Website regarding prime numbers. What the visualization shows is that if you draw periodic curves beginning at the origin for each positive integer, the prime numbers will be intersected by only two curves: the prime itself&amp;rsquo;s curve and the curve for one.
When I saw this, my mind was blown. How interesting&amp;hellip; and also how obvious. The definition of a prime is that it can only be divided by itself and one (duh).</description>
    </item>
    
    <item>
      <title>Exploring Open Data - Predicting the Amoung of Violations</title>
      <link>/post/exploring-open-data-predicting-the-amount-of-violations/</link>
      <pubDate>Tue, 15 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-open-data-predicting-the-amount-of-violations/</guid>
      <description>Introduction In my last post, I went over some of the highlights of the open data set of all Philadelphia Parking Violations. In this post, I’ll go through the steps to build a model to predict the amount of violations the city issues on a daily basis. I’ll walk you through cleaning and building the data set, selecting and creating the important features, and building predictive models using Random Forests and Linear Regression.</description>
    </item>
    
    <item>
      <title>Using R and Splunk: Lookups of More Than 10,000 Results</title>
      <link>/post/using-r-and-splunk/</link>
      <pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/using-r-and-splunk/</guid>
      <description>Splunk, for some probably very good reasons, has limits on how many results are returned by sub-searches (which in turn limits us on lookups, too). Because of this, I’ve used R to search Splunk through it’s API endpoints (using the httr package) and utilize loops, the plyr package, and other data manipulation flexibilities given through the use of R.
This has allowed me to answer some questions for our business team that at the surface seem simple enough, but the data gathering and manipulation get either too complex or large for Splunk to handle efficiently.</description>
    </item>
    
    <item>
      <title>Using the Google Search API and Plotly to Locate Waterparks</title>
      <link>/post/waterparks-with-the-google-search-api-and-plotly/</link>
      <pubDate>Wed, 04 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/waterparks-with-the-google-search-api-and-plotly/</guid>
      <description>I’ve got a buddy who manages and builds waterparks. I thought to myself… I am probably the only person in the world who has a friend that works at a waterpark - cool. Then I started thinking some more… there has to be more than just his waterpark in this country; I’ve been to at least a few… and the thinking continued… I wonder how many there are… and continued… and I wonder where they are… and, well, here we are at the culmination of that curiosity with this blog post.</description>
    </item>
    
    <item>
      <title>Sierpinski Triangles (and Carpets) in R</title>
      <link>/post/sierpinski-triangles-and-carpets-in-r/</link>
      <pubDate>Mon, 05 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/sierpinski-triangles-and-carpets-in-r/</guid>
      <description>Recently in class, I was asked the following question:
 Start with an equilateral triangle and a point chosen at random from the interior of that triangle. Label one vertex 1, 2, a second vertex 3, 4, and the last vertex 5, 6. Roll a die to pick a vertex. Place a dot at the point halfway between the roll-selected vertex and the point you chose. Now consider this new dot as a starting point to do this experiment once again.</description>
    </item>
    
    <item>
      <title>Identifying Compromised User Accounts with Logistic Regression</title>
      <link>/post/identifying-compromised-user-accounts/</link>
      <pubDate>Sat, 26 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/identifying-compromised-user-accounts/</guid>
      <description>INTRODUCTION As a Data Analyst on Comcast&amp;rsquo;s Messaging Engineering team, it is my responsibility to report on the platform statuses, identify irregularities, measure impact of changes, and identify policies to ensure that our system is used as it was intended. Part of the last responsibility is the identification and remediation of compromised user accounts.
The challenge the company faces is being able to detect account compromises faster and remediate them closer to the moment of detection.</description>
    </item>
    
    <item>
      <title>Doing a Sentiment Analysis on Tweets (Part 2)</title>
      <link>/post/doing-a-sentiment-analysis-on-tweets-part-2/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/doing-a-sentiment-analysis-on-tweets-part-2/</guid>
      <description>INTRO This is post is a continuation of my last post. There I pulled tweets from Twitter related to “Comcast email,” got rid of the junk, and removed the unnecessary/unwanted data.
Now that I have the tweets, I will further clean the text and subject it to two different analyses: emotion and polarity.
 WHY DOES THIS MATTER Before I get started, I thought it might be a good idea to talk about WHY I am doing this (besides the fact that I learned a new skill and want to show it off and get feedback).</description>
    </item>
    
    <item>
      <title>Doing a Sentiment Analysis on Tweets (Part 1)</title>
      <link>/post/doing-a-sentiment-analysis-on-tweets-part-1/</link>
      <pubDate>Wed, 24 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/doing-a-sentiment-analysis-on-tweets-part-1/</guid>
      <description>INTRO So… This post is my first foray into the R twitteR package. This post assumes that you have that package installed already in R. I show here how to get tweets from Twitter in preparation for doing some sentiment analysis. My next post will be the actual sentiment analysis.
For this example, I am grabbing tweets related to “Comcast email.” My goal of this exercise is to see how people are feeling about the product I support.</description>
    </item>
    
  </channel>
</rss>