<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jake Learns Data Science</title>
    <link>/post/</link>
    <description>Recent content in Posts on Jake Learns Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Jacob Rozran</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Data Scientist&#39;s Take on the Roadmap to AI</title>
      <link>/post/a-data-scientists-take-on-the-roadmap-to-ai/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-data-scientists-take-on-the-roadmap-to-ai/</guid>
      <description>INTRODUCTION Recently I was asked by a former colleague about getting into AI. He has truly big data and wants to use this data to power “AI” - if the headlines are to be believed, everyone else is already doing it. Though it was difficult for my ego, I told him I couldn’t help him in our 30 minute call and that he should think about hiring someone to get him there.</description>
    </item>
    
    <item>
      <title>Visualizing Exercise Data from Strava</title>
      <link>/post/visualizing-exercise-data-from-strava/</link>
      <pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-exercise-data-from-strava/</guid>
      <description>INTRODUCTION My wife introduced me to cycling in 2014 - I fell in love with it and went all in. That first summer after buying my bike, I rode over 500 miles (more on that below). My neighbors at the time, also cyclists, introduced me to the app Strava. Ever since then, I’ve tracked all of my rides, runs, hikes, walks (perhaps not really exercise that needs to be tracked… but I hurt myself early in 2018 and that’s all I could do for a while), etc.</description>
    </item>
    
    <item>
      <title>Getting Started with Data Science</title>
      <link>/post/getting-started-with-data-science/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/getting-started-with-data-science/</guid>
      <description>Everyone in the world has a &amp;ldquo;how to&amp;rdquo; guide to data science&amp;hellip; well, maybe not everyone - but there are a lot of &amp;ldquo;guides&amp;rdquo; out there. I get this question infrequently, so I thought I would do my best to put together what have been my best resources for learning.
MY STORY Personally, I learned statistics by getting my Masters in Applied Statistics at Villanova University - it took 2.</description>
    </item>
    
    <item>
      <title>How to Build a Binary Classification Model</title>
      <link>/post/how-to-build-a-binary-classification-model/</link>
      <pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/how-to-build-a-binary-classification-model/</guid>
      <description>What Is Binary Classification? Algorithms for Binary Classification Logistic Regression Decision Trees/Random Forests Decision Trees Random Forests  Nearest Neighbor Support Vector Machines (SVM) Neural Networks  Great. Now what? Determining What the Problem is Locate and Obtain Data Data Mining &amp;amp; Preparing for Analysis Splitting the Data Building the Models Validating the Models  Conclusion   What Is Binary Classification? Binary classification is used to classify a given set into two categories.</description>
    </item>
    
    <item>
      <title>Prime Number Patterns</title>
      <link>/post/prime-number-patterns/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/prime-number-patterns/</guid>
      <description>I found a very thought provoking and beautiful visualization on the D3 Website regarding prime numbers. What the visualization shows is that if you draw periodic curves beginning at the origin for each positive integer, the prime numbers will be intersected by only two curves: the prime itself&amp;rsquo;s curve and the curve for one.
When I saw this, my mind was blown. How interesting&amp;hellip; and also how obvious. The definition of a prime is that it can only be divided by itself and one (duh).</description>
    </item>
    
    <item>
      <title>Machine Learning Demystified</title>
      <link>/post/machine-learning-demystified/</link>
      <pubDate>Thu, 07 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/machine-learning-demystified/</guid>
      <description>The differences and applications of Supervised and Unsupervised Machine Learning.
Introduction Machine learning is one of the buzziest terms thrown around in technology these days. Combine machine learning with big data in a Google search and you&amp;rsquo;ve got yourself an unmanageable amount of information to digest. In an (possibly ironic) effort to help navigate this sea of information, this post is meant to be an introduction and simplification of some common machine learning terminology and types with some resources to dive deeper.</description>
    </item>
    
    <item>
      <title>Exploring Open Data - Predicting the Amoung of Violations</title>
      <link>/post/exploring-open-data-predicting-the-amount-of-violations/</link>
      <pubDate>Tue, 15 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-open-data-predicting-the-amount-of-violations/</guid>
      <description>Introduction In my last post, I went over some of the highlights of the open data set of all Philadelphia Parking Violations. In this post, I’ll go through the steps to build a model to predict the amount of violations the city issues on a daily basis. I’ll walk you through cleaning and building the data set, selecting and creating the important features, and building predictive models using Random Forests and Linear Regression.</description>
    </item>
    
    <item>
      <title>Exploring Open Data - Philadelphia Parking Violations</title>
      <link>/post/exploring-open-data-philadelphia-parking-violations/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/exploring-open-data-philadelphia-parking-violations/</guid>
      <description>Introduction A few weeks ago, I stumbled across Dylan Purcell&amp;rsquo;s article on Philadelphia Parking Violations. This is a nice glimpse of the data, but I wanted to get a taste of it myself. I went and downloaded the entire data set of Parking Violations in Philadelphia from the OpenDataPhilly website and came up with a few questions after checking out the data:
How many tickets in the data set?</description>
    </item>
    
    <item>
      <title>Open Data Day - DC Hackathon</title>
      <link>/post/open-data-day-dc-hackathon/</link>
      <pubDate>Mon, 07 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/open-data-day-dc-hackathon/</guid>
      <description>For those of you who aren&amp;rsquo;t stirred from bed in the small hours to learn data science, you might have missed that March 5th was international open data day. There are hundreds of local events around the world; I was lucky enough to attend DC&amp;rsquo;s Open Data Day Hackathon. I met a bunch of great people doing noble things with data who taught me a crap-ton (scientific term) and also validated my love for data science and how much I&amp;rsquo;ve learned since beginning my journey almost two years ago.</description>
    </item>
    
    <item>
      <title>Using R and Splunk: Lookups of More Than 10,000 Results</title>
      <link>/post/using-r-and-splunk/</link>
      <pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/using-r-and-splunk/</guid>
      <description>Splunk, for some probably very good reasons, has limits on how many results are returned by sub-searches (which in turn limits us on lookups, too). Because of this, I’ve used R to search Splunk through it’s API endpoints (using the httr package) and utilize loops, the plyr package, and other data manipulation flexibilities given through the use of R.
This has allowed me to answer some questions for our business team that at the surface seem simple enough, but the data gathering and manipulation get either too complex or large for Splunk to handle efficiently.</description>
    </item>
    
    <item>
      <title>Using the Google Search API and Plotly to Locate Waterparks</title>
      <link>/post/waterparks-with-the-google-search-api-and-plotly/</link>
      <pubDate>Wed, 04 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/waterparks-with-the-google-search-api-and-plotly/</guid>
      <description>I’ve got a buddy who manages and builds waterparks. I thought to myself… I am probably the only person in the world who has a friend that works at a waterpark - cool. Then I started thinking some more… there has to be more than just his waterpark in this country; I’ve been to at least a few… and the thinking continued… I wonder how many there are… and continued… and I wonder where they are… and, well, here we are at the culmination of that curiosity with this blog post.</description>
    </item>
    
    <item>
      <title>Sierpinski Triangles (and Carpets) in R</title>
      <link>/post/sierpinski-triangles-and-carpets-in-r/</link>
      <pubDate>Mon, 05 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/sierpinski-triangles-and-carpets-in-r/</guid>
      <description>Recently in class, I was asked the following question:
 Start with an equilateral triangle and a point chosen at random from the interior of that triangle. Label one vertex 1, 2, a second vertex 3, 4, and the last vertex 5, 6. Roll a die to pick a vertex. Place a dot at the point halfway between the roll-selected vertex and the point you chose. Now consider this new dot as a starting point to do this experiment once again.</description>
    </item>
    
    <item>
      <title>Identifying Compromised User Accounts with Logistic Regression</title>
      <link>/post/identifying-compromised-user-accounts/</link>
      <pubDate>Sat, 26 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/identifying-compromised-user-accounts/</guid>
      <description>INTRODUCTION As a Data Analyst on Comcast&amp;rsquo;s Messaging Engineering team, it is my responsibility to report on the platform statuses, identify irregularities, measure impact of changes, and identify policies to ensure that our system is used as it was intended. Part of the last responsibility is the identification and remediation of compromised user accounts.
The challenge the company faces is being able to detect account compromises faster and remediate them closer to the moment of detection.</description>
    </item>
    
    <item>
      <title>First Annual Data Jawn</title>
      <link>/post/first-annual-data-jawn/</link>
      <pubDate>Thu, 16 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/first-annual-data-jawn/</guid>
      <description>I went to a data event last night called the Data Jawn, presented by RJMetrics, and wanted to share a write up of the event – it was pretty cool and sparked some really good ideas.
One idea I really liked – Predicting who will call in/have an issue and proactively reaching out to them… Sounds like a game changer to me. I could then trigger alerts when we see that activity or make a list for reaching out via phone, email, text… whatever.</description>
    </item>
    
    <item>
      <title>Understanding User Agents</title>
      <link>/post/understanding-user-agents/</link>
      <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/understanding-user-agents/</guid>
      <description>INTRODUCTION I have had a few discussions around web user agents at work recently. It turns out that they are not straightforward at all. In other words, trying to report browser usage to our Business Unit required a nontrivial translation. The more I dug in, the more I learned. I had some challenges finding the information, so I thought it be useful to document my findings and centralizing the sites I used to figure all this out.</description>
    </item>
    
    <item>
      <title>Doing a Sentiment Analysis on Tweets (Part 2)</title>
      <link>/post/doing-a-sentiment-analysis-on-tweets-part-2/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/doing-a-sentiment-analysis-on-tweets-part-2/</guid>
      <description>INTRO This is post is a continuation of my last post. There I pulled tweets from Twitter related to “Comcast email,” got rid of the junk, and removed the unnecessary/unwanted data.
Now that I have the tweets, I will further clean the text and subject it to two different analyses: emotion and polarity.
 WHY DOES THIS MATTER Before I get started, I thought it might be a good idea to talk about WHY I am doing this (besides the fact that I learned a new skill and want to show it off and get feedback).</description>
    </item>
    
    <item>
      <title>Doing a Sentiment Analysis on Tweets (Part 1)</title>
      <link>/post/doing-a-sentiment-analysis-on-tweets-part-1/</link>
      <pubDate>Wed, 24 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/doing-a-sentiment-analysis-on-tweets-part-1/</guid>
      <description>INTRO So… This post is my first foray into the R twitteR package. This post assumes that you have that package installed already in R. I show here how to get tweets from Twitter in preparation for doing some sentiment analysis. My next post will be the actual sentiment analysis.
For this example, I am grabbing tweets related to “Comcast email.” My goal of this exercise is to see how people are feeling about the product I support.</description>
    </item>
    
  </channel>
</rss>